Messaging System Internal Data Striucture
    1. Queue Based DS - AWS SQS, Rabit MQ etc.
    2. Log Based DS - Kafka, AWS Kinesis etc.
        Log is the simplest data structure that represents append-only sequence of records. Log records are immutable
        and appended in strictly sequential order to the end of the log file.

SQS vs Kafka

Amazon Simple Queue Service

    1. SQS is a managed message queuing service that we use to send multiple messages of various sizes and received by another server.

    2. Importantly SQS make developers free from the hassle of configuring and managing queue structures as a managed
       service. Especially this service provides you with everything you need out of the box and is easily scalable to
       meet your demand.

    3. In SQS users cannot decide which messages in the queue to receive. However, you can limit the number of messages
     you will receive at any one time.

    4. SQS uses a blocking mechanism, if one component consumes a message, it  hides it from other components.

    5. When a message is received, SQS temporarily removes it from the queues so it is never sent twice. Moreover the
     message is hidden for a while and the user processes it and remove it from the list.

    6. If the message is not removed after the visibility timeout, it is then returned to the queue and receives a
     future requests. Timeout interval for visibility can be set.

    7. Each request is handled independently.

    8. You are paying for the number of orders in the queue.
    9. Excellent service that improves the efficiency, reliability and performance of applications.
    10. Messages in the queue are delivered at least once. So guaranteed message delivery without message loss.
    11. hose messages that cannot be processed are saved in the garbage queue.
    12. There are two types of queues: standard queues and FIFO queues. In the default queue, messages are retrieved randomly.


Kafka

    1. Apache Kafka is an open source distributed messaging platform. Designed to manage streaming data in real time for
    distributed systems.

    2. Apache Kafka is, of course, an optimized data warehouse in terms of data transfer and processing. The transferred
    data is continuously generated by thousands of data sources, typically by sending data records. Streaming platforms
    have to deal with the continuous flow of data, sequentially and incrementally.

    3. Kafka is one of the fastest growing open source messaging communications solutions.

    4. Distributed, partitioned, replicated and fault tolerant, leads to amazing reliability.
    5. Replication messages are replicated across multiple data centers or cloud regions.
    6. Durable solution as it uses distributed commit log, that means messages persists on disk as fast as possible.
    7. Clusters of Kafka handle large failures and databases.


Kafka vs SQS - Key Differences

    Kafka is described in detail as a “fault tolerant and high performance publish subscribe messaging system.” So
    Kafka is a distributed, segmented and redundant commit log service. Provides the functionality of the email system
    but with a unique design.

    Max message size - 1 MB & configurable
    Message order always preserved
    Data retention is also configurable. Default 7 days.

    Amazon SQS as a “fully managed message queue service”. Hence you can transfer any amount of data without dropping
    messages or requiring other services to be always available. With SQS, you reduce the administrative burden of
    running and scaling a highly available message pool. While getting what you use at an affordable price.

    Max message size - 256 kb
    Message order always preserved only for FIFO queue
    Data retention 60 secs to 14 days


    Kafka Use cases

        Stream Data processing framework.
        Highly scalable system for large workloads that need to send messages in batches
        Topics in Kafka consists of multiple sections that are read in parallel by different consumers of a group of
        consumers, which gives us a very good performance.

    SQS Use cases

        Better suited for events, when you need to intercept a message (event) from the client, the message is
        automatically pulled from the queue.
        Not as fast as Kafka and not suitable for large workloads.
        Better for events with few events per second.

    Message Model - Apache Kafka follows publish subscriber model, where as SQS is pull based streaming.

    SQS has two types of queues: FIFO and Standard

    Message size - For Kafka is 1MB and configurable. But for SQS Max Message size 256KB.



Why is Kafka so fast?

We will go through 2 very important factors that contribute to it:
    a. Sequential I/O
    b. Zero Copy Principle

Sequential I/O

One of the key factors that contributes to its performance is its use of sequential I/O. It uses sequential I/O to
improve the performance of its log-based storage system.

Kafka stores all the data it ingests in a log-structured format, which means that new data is appended to the end of the
log. This allows for very fast writes, since only the latest data needs to be appended, but it also means that older
data needs to be periodically compacted to reclaim disk space.

In addition to this, Kafka also uses a technique called log compaction to clean up old data, which is also done in a
sequential manner. It keeps the latest version of a message key, while discarding older versions, thus making use of
the sequential IO when doing compaction.

Zero-Copy Principle

The zero-copy principle is a technique used in computer systems to minimise the number of times data is copied between
different memory locations. This can significantly improve performance by reducing the amount of memory and CPU
resources used.

When data needs to be transferred from one location to another, a traditional approach would be to copy the data from
the source location to a temporary buffer, and then copy it again to the destination location. However, the zero-copy
principle eliminates this intermediate step by allowing the data to be transferred directly from the source location to
the destination location, without the need for an intermediate copy.

There are different ways to implement the zero-copy principle, but some common techniques include:
Memory mapping
Direct memory access (DMA)

Kafka uses the zero-copy principle to improve its performance by minimising the number of copies of data that are made
as messages are produced and consumed.

When a consumer reads a message from a Kafka topic, it can read the data directly from the log file, without the need
to copy it into a separate buffer. This is achieved using a technique called direct memory access (DMA). DMA allows the
consumer to read the data directly from the log file into the consumer’s memory buffer, without the need for an intermediate copy.


Kafka Partitions
Definition:
Partitions are a fundamental unit of parallelism in Kafka. Each topic in Kafka is divided into one or more partitions.

Key Characteristics:

Scalability: By splitting a topic into multiple partitions, Kafka can distribute the load across multiple brokers, making it possible to handle larger volumes of data.
Order Guarantee: Within a partition, records are strictly ordered.
Parallelism: Different partitions can be consumed by different consumers in parallel, which allows for increased throughput.

Kafka Consumer Groups
Definition:
A consumer group is a group of consumers that work together to consume records from a set of Kafka partitions.

Key Characteristics:

Load Balancing: Kafka automatically balances the load of message consumption among the consumers in the same group. Each partition in the topic is assigned to exactly one consumer in the group at any time, ensuring that no two consumers read from the same partition.
Fault Tolerance: If a consumer instance in a group fails, Kafka will automatically rebalance the partitions assigned to the failed consumer across the remaining active consumers in the group. This ensures that the message consumption continues without interruption.
Independent Offset Management: Each consumer group maintains its own offsets, which are the positions of the consumers within the partitions of a topic. This allows different consumer groups to consume the same topic independently and at different rates.


How They Work Together
Partition Assignment:

When a consumer group subscribes to a topic, Kafka assigns partitions to consumers in that group.

Parallel Processing:

Each partition can be processed independently by a different consumer within the group, enabling parallel processing and increasing throughput.

Rebalancing:

If a consumer joins or leaves the group, or if a new partition is added to the topic, Kafka will trigger a rebalance. During rebalancing, Kafka redistributes the partitions among the available consumers to maintain an even load.
Offset Management:

Kafka stores the offsets of the messages each consumer group has processed in a special internal topic called __consumer_offsets. This allows each consumer group to resume processing from the last committed offset in case of a restart.